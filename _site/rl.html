<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <link rel="preload" href="min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="icon" href="favicon.svg">
    
    <link defer rel="stylesheet" href="min.css">
    <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">
    
    <!-- Katex -->
    <script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
    <script>document.addEventListener("DOMContentLoaded", function () {
      var mathElements = document.getElementsByClassName("math");
      var macros = [];
      for (var i = 0; i < mathElements.length; i++) {
        var texText = mathElements[i].firstChild;
        if (mathElements[i].tagName == "SPAN") {
        katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }}});
    </script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
    
    <meta name="date" content=''>
    <title>Reinforcement Learning</title>
  </head>
  <body>
    <h1><a href="/"a>← Reinforcement Learning</a></h1>
    <p class="date">Dec 22, 2023 &ndash; Dec 26, 2023</p>
    <main>
      <p>Reinforcement Learning (RL) is a paradigm of ML where instead
      of telling the AI what is correct, there is a notion of getting
      “rewards” for your actions. The general goal is for your AI to
      maximize your return (reward over the long run).</p>
      <p>As of now, this is just a bookmark page to store resources and
      links to other notebooks! Here are some of my other notes on RL
      topics:</p>
      <ul>
      <li><a href="monte-carlo-tree-search">Monte Carlo Tree
      Search</a></li>
      <li><a href="policy-gradients">Policy Gradients</a></li>
      </ul>
      <h2 id="questions">Questions</h2>
      <ul>
      <li><strong>How can you do self-critiquing RL on supervised models
      <em>without</em> collapse?</strong> So something to be careful of
      is you don’t want them to just output the mean like that one paper
      where the distribution collapsed. If you have a model
      self-supervise, then how can it improve?
      <ul>
      <li>A lot of open-source models use GPT-4 as a ground truth
      because it’s better, but how could GPT-4 use itself to improve?
      It’s hard because not like MCTS where there is a perfect
      environment simulator for language. That doesn’t exist. There has
      been quite a lot of work in exploring LLMs are a multiverse idea,
      but for you to decide the <em>best</em> one, you have to assign
      meaning. This reminds me of a <a href="biology">biology</a>
      concept where the definition of a living thing is something that
      creates meaning in the world.</li>
      <li>What if your ground-truth were just the training examples of a
      dataset? Also not sure how you MCTS when there are ~50, 000 tokens
      that could come after and the tree would be absolutely massive.
      MCTS on language sounds <em>extremely expensive</em>.
      <strong>But</strong>, if you could, then you want the policies
      that lead to the right answer most of the time. This might have
      the same overfitting policy, but if you have a robust enough
      training dataset, I’m not sure why this couldn’t work.</li>
      </ul></li>
      </ul>
      <h2 id="references">References</h2>
      <ul>
      <li><a
      href="http://incompleteideas.net/book/the-book-2nd.html">Sutton
      &amp; Barto</a></li>
      <li><a
      href="https://spinningup.openai.com/en/latest/index.html">Spinning
      Up</a></li>
      </ul>
      <h2 id="to-read">To Read</h2>
      <ul>
      <li><a
      href="https://gist.github.com/knewjade/24fd3a655e5321c8ebac8b93fa497ed9">KnewJade
      Hatetris writeup</a></li>
      <li><a href="https://arxiv.org/abs/2302.08242">Tuning computer
      vision models with task rewards</a></li>
      </ul>
    </main>
    <script src="//instant.page/5.1.1" type="module" integrity="sha384-MWfCL6g1OTGsbSwfuMHc8+8J2u71/LA8dzlIN3ycajckxuZZmF+DNjdm7O6H3PSq"></script>
  </body>
</html>
