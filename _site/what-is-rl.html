<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <link rel="preload" href="min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="icon" href="favicon.svg">
    
    <link defer rel="stylesheet" href="min.css">
    <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">
    
    <!-- Katex -->
    <script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
    <script>document.addEventListener("DOMContentLoaded", function () {
      var mathElements = document.getElementsByClassName("math");
      var macros = [];
      for (var i = 0; i < mathElements.length; i++) {
        var texText = mathElements[i].firstChild;
        if (mathElements[i].tagName == "SPAN") {
        katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }}});
    </script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
    
    <meta name="date" content=''>
    <title>#1: What is RL?</title>
  </head>
  <body>
    <h1><a href="/"a>← #1: What is RL?</a></h1>
    <p class="date">May 31, 2024 &ndash; May 31, 2024</p>
    <main>
      <p>In AI, there are three broad areas of how we get machines to
      learn: supervised learning, unsupervised learning, and
      reinforcement learning. Rather than directly jump into RL, let’s
      first explore the other 2 to see how reinforcement learning
      differs.</p>
      <figure>
      <img src="_assets/supervised-learning.png"
      alt="Supervised Learning" />
      <figcaption aria-hidden="true">Supervised Learning</figcaption>
      </figure>
      <p>In supervised learning, we give input &amp; output pairs and
      tell the AI what the correct answer is. Through thousands to
      trillions of these examples, models slowly learn a function to map
      between the two.</p>
      <figure>
      <img src="_assets/unsupervised-learning.png"
      alt="Unsupervised Learning" />
      <figcaption aria-hidden="true">Unsupervised Learning</figcaption>
      </figure>
      <p>In unsupervised learning, rather than telling a model what’s
      correct, we want it to learn implicit representations of the data.
      This comes in lots of different forms, but the key difference is
      that no human is giving some “correct answer”. Rather, we find
      clever ways for algorithms to learn how to group different
      inputs.</p>
      <p>Finally in reinforcement learning, we’re interested in not just
      passive predictions, but agents that can take actions in a world.
      Specifically, we’re trying to learn how to map from states (what
      is the world like now?) to actions (what should I do?).</p>
      <h2 id="formalisms">Formalisms</h2>
      <p><img src="_assets/shapes%20at%2024-05-30%2015.31.57.png" /></p>
      <p>In RL, we have an agent and environment. The agent is anything
      that we can arbitrarily control, so this might be the motors on a
      robot or the arms of a human. Anything that is outside of this is
      NOT within the agent (for example, dopamine is not within you as
      an agent since you can’t change it at will). On each time step,
      the agent does an action and the environment returns the new world
      state along with a reward. Rather than learning some direct
      mapping from input to output, we’re instead trying to map states
      to a guess of <em>value</em> or how much reward we’ll receive over
      time.</p>
      <blockquote>
      <p><strong>IMPORTANT NOTE:</strong> We are always trying to
      maximize value (expected return over time) and not reward. It
      isn’t about the action which gives us the most reward immediately
      but which actions and states will give us the most success over
      time.</p>
      </blockquote>
      <p>You might also notice something odd in that our reward is just
      a singular number. This is because a central hypothesis in
      reinforcement learning is that: <strong>all goal directed behavior
      is motivated by a single number.</strong> I think it’s worth the
      time to pause and see whether or not you agree with this. Are
      there scenarios where this might break down? Are there ways to
      reconcile that? No matter what RL methods you’re trying to use,
      you’re constantly just trying to maximize/minimize a singular
      scalar.</p>
      <p>So what do you get with all these formalisms? You get world
      class chess/go AIs, Youtube video delivery optimization, and
      robots that can navigate around spaces. This kind of learning is
      very broad, and over the next few explainers, I hope to show some
      of the depth of what these techniques can do.</p>
    </main>
    <script src="//instant.page/5.1.1" type="module" integrity="sha384-MWfCL6g1OTGsbSwfuMHc8+8J2u71/LA8dzlIN3ycajckxuZZmF+DNjdm7O6H3PSq"></script>
  </body>
</html>
